# -*- coding: utf-8 -*-
"""OCR_HANDWRITING_CNTT2_FINALLY.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ImRaH1bLzlCTyHYMk_NqxqF0LrY__wak

# Vietnamese Handwritten Recognition with CRNN model

## Load Data
"""

import tensorflow as tf
from google.colab import drive
drive.mount('/content/gdrive')
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  # RIP Training time
  raise SystemError('GPU device not found')
  print('Found GPU at: {}'.format(device_name))

import os
# path to our directory images

DATA_PATH = '/content/gdrive/MyDrive/CNTT2/DATASET/raw/data'
DATA_SELF_GENERATED_PATH = '/content/gdrive/MyDrive/CNTT2/DATASET/raw/self_generated_data'

TEST_FOLDER = '/content/gdrive/MyDrive/CNTT2/DATASET/raw/test'
TRAIN_FOLDER = '/content/gdrive/MyDrive/CNTT2/DATASET/raw/train'

TRAIN_JSON = '/content/gdrive/MyDrive/CNTT2/DATASET/raw/labels.json'
SELF_GENERATED_TRAIN_JSON = '/content/gdrive/MyDrive/CNTT2/DATASET/raw/self_generated_labels.json'

BEST_CHECKPOINT_WEIGHT = '/content/gdrive/MyDrive/CNTT2/DATASET/model/crnn/best_weights_epochs.hdf5'
LOG_TENSORBOARD = '/content/gdrive/MyDrive/CNTT2/DATASET/model/crnn/logs'

STRUCTURE_MODEL_IMG = '/content/gdrive/MyDrive/CNTT2/DATASET/model/crnn/model.png'

data_paths = [DATA_PATH, DATA_SELF_GENERATED_PATH]

import json
# open these label jsons files
train_labels = {}
self_train_labels = {}
with open(TRAIN_JSON, 'r', encoding='utf8') as f:
  train_labels = json.load(f)

with open(SELF_GENERATED_TRAIN_JSON, 'r', encoding='utf8') as f:
  self_train_labels = json.load(f)

# join two objects
train_labels.update(self_train_labels)

train_labels

#find all characters in labels
char_list = set()
for label in train_labels.values():
  char_list.update(set(label))
char_list = sorted(char_list)
len(char_list)

# show all possible labels characters
"".join(char_list)

# convert the words to array of indexs based on the char_list
def encode_to_labels(txt):
  # encoding each output word into digits of indexes
  convert = []
  for index, char in enumerate(txt):
    try:
      convert.append(char_list.index(char))
    except:
      print("No found in char_list :", char)

  return convert

encode_to_labels("Huỳnh Tấn Lợi - fine")

from pathlib import Path

train_image_path = []

for path in data_paths:
  train_image_path.extend([str(item) for item in Path(path).glob('*')])

len(train_image_path)

train_image_path[:10]

"""## Preprocessing
- Find all widths and heights of images
- Use openCV to read image
- Preprocess images (like converting images to greyscale)
- Resize images so all images will have the same size
- Split your dataset into trainset and testset
- Build CRNN model with CTC loss
- Prediction
- Calculate metrics for SER, WER and CER
"""

# Matching full absoluate paths and labels instead of filenames and labels
dict_filepath_label = {}

for path in data_paths:
  raw_data_path = Path(path)
  for item in raw_data_path.glob('*.*'):
    file_name = str(os.path.basename(item))
    label = train_labels[file_name]
    dict_filepath_label[str(item)] = label

len(dict_filepath_label)

dict_filepath_label

# find the maximum label length
max_label_len = max(len(label) for label in dict_filepath_label.values())

max_label_len

all_image_paths = list(dict_filepath_label.keys())

# last 10 elements
all_image_paths[-10:]

import cv2
# find all widths and heights of images (this is useful if our dataset images got different sizes)
widths = []
heights = []
for image_path in all_image_paths:
  img = cv2.imread(image_path)
  (height, width, _) = img.shape
  heights.append(height)
  widths.append(width)

min_height = min(heights)
max_height = max(heights)
min_width = min(widths)
max_width = max(widths)

(min_height, max_height, min_width, max_width)

# Train test diversity with the ratio 8/2
from sklearn.model_selection import train_test_split
test_size = 0.2
train_image_paths, val_image_paths = train_test_split(all_image_paths, test_size=test_size, random_state=42)

TIME_STEPS = 240

"""### Preprocessing function"""

import numpy as np
from PIL import Image, ImageEnhance, ImageFilter

def preprocessing_img(img_paths, pre_img, pre_text_img, input_length, label_length, orig_txt):
  resize_max_width = 0
  i = 0
  for train_img_path in img_paths:
    img = Image.open(train_img_path)

    # Bước 1: Tăng cường độ tương phản
    enhancer_contrast = ImageEnhance.Contrast(img)
    img = enhancer_contrast.enhance(1.5)

    # Bước 2: Làm rõ văn bản bằng kỹ thuật xử lý lọc
    img = img.filter(ImageFilter.MedianFilter(size=3))

    # Bước 3: Tăng cường độ sáng
    enhancer_brightness = ImageEnhance.Brightness(img)
    img = enhancer_brightness.enhance(1.2)
    # read input image and convert into gray scale image
    img = cv2.cvtColor(np.array(img), cv2.COLOR_BGR2GRAY)

    height, width = img.shape

    # in this dataset, we don't need to do any resize at all here.
    img = cv2.resize(img,(int(118/height*width),118))

    height, width = img.shape

    if img.shape[1] > resize_max_width:
        resize_max_width = img.shape[1]

    # Padding image using median
    img = np.pad(img, ((0,0),(0, 2167-width)), 'median')

    # Blur it
    img = cv2.GaussianBlur(img, (5,5), 0)

    # Threshold the image using adapative threshold - binary images
    img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 4)

    # add channel dimension
    img = np.expand_dims(img , axis = 2)

    # Normalizes the pixel value of the image to the range from 0 to 1.
    img = img/255.

    label = dict_filepath_label[train_img_path]

    # split data into validation and training dataset as 10% and 90% respectively
    orig_txt.append(label)
    label_length.append(len(label))

    # our time steps for valid input
    input_length.append(TIME_STEPS)
    pre_img.append(img)

    # convert words to digits based on charlist
    pre_text_img.append(encode_to_labels(label))
    i += 1
    if (i%500 == 0):
      print ("has processed trained {} files".format(i))

  print(resize_max_width)
  return pre_img, pre_text_img, input_length, label_length, orig_txt

"""### Preprocessing - training dataset"""

# lists for training dataset
training_img = []
training_txt = []
train_input_length = []
train_label_length = []
orig_txt = []

training_img, training_txt, train_input_length, train_label_length, orig_txt = preprocessing_img(
    train_image_paths,
    training_img,
    training_txt,
    train_input_length,
    train_label_length,
    orig_txt
    )

len(training_img)

import matplotlib.pyplot as plt
for i in range(len(training_img)-10, len(training_img)):
  plt.figure(figsize=(15,2))
  plt.imshow(training_img[i][:,:,0], cmap="gray")
  plt.show()

"""### Preprocessing - valid dataset"""

#lists for validation dataset
valid_img = []
valid_txt = []
valid_input_length = []
valid_label_length = []
valid_orig_txt = []

valid_img, valid_txt, valid_input_length, valid_label_length, valid_orig_txt = preprocessing_img(
    val_image_paths,
    valid_img,
    valid_txt,
    valid_input_length,
    valid_label_length,
    valid_orig_txt
    )

len(valid_img)

import matplotlib.pyplot as plt
for i in range(len(valid_img)-10, len(valid_img)):
  plt.figure(figsize=(15,2))
  plt.imshow(valid_img[i][:,:,0], cmap="gray")
  plt.show()

max_label_len = TIME_STEPS

# import padding library
from keras.utils import pad_sequences

# pad each output label to maximum text length, remember we did that so that we keep training with rnn consistent?
train_padded_txt = pad_sequences(training_txt, maxlen = max_label_len, padding='post', value = 0)
valid_padded_txt = pad_sequences(valid_txt, maxlen = max_label_len, padding='post', value = 0)

train_padded_txt[0]

"""## Model Building"""

# import our model, different layers and activation function
from keras.layers import Layer, Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional, Add, Activation, RepeatVector, Permute, multiply, InputSpec
from keras.models import Model
from keras.activations import relu, sigmoid, softmax
import keras.backend as K
from keras.utils import to_categorical, plot_model
from keras.callbacks import CSVLogger, TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau

"""
### Structure"""

# input with shape of height=32 and width=128
inputs = Input(shape=(118,2167,1))

# Block 1
x = Conv2D(64, (3,3), padding='same')(inputs)
x = MaxPool2D(pool_size=3, strides=3)(x)
x = Activation('relu')(x)
x_1 = x

# Block 2
x = Conv2D(128, (3,3), padding='same')(x)
x = MaxPool2D(pool_size=3, strides=3)(x)
x = Activation('relu')(x)
x_2 = x

# Block 3
x = Conv2D(256, (3,3), padding='same')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x_3 = x

# Block4
x = Conv2D(256, (3,3), padding='same')(x)
x = BatchNormalization()(x)
x = Add()([x,x_3])
x = Activation('relu')(x)
x_4 = x

# Block5
x = Conv2D(512, (3,3), padding='same')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x_5 = x

# Block6
x = Conv2D(512, (3,3), padding='same')(x)
x = BatchNormalization()(x)
x = Add()([x,x_5])
x = Activation('relu')(x)

# Block7
x = Conv2D(1024, (3,3), padding='same')(x)
x = BatchNormalization()(x)
x = MaxPool2D(pool_size=(3, 1))(x)
x = Activation('relu')(x)

# pooling layer with kernel size (3,1) to make the height/3 #(1,9,512) to (1, 3, 512)
x = MaxPool2D(pool_size=(3, 1))(x)

# # to remove the first dimension of one: (1, 3, 512) to (3, 512)
squeezed = Lambda(lambda x: K.squeeze(x, 1))(x)

# # # bidirectional LSTM layers with units=128
blstm_1 = Bidirectional(LSTM(512, return_sequences=True, dropout=0.25))(squeezed)
blstm_2 = Bidirectional(LSTM(512, return_sequences=True, dropout=0.25))(blstm_1)

# # this is our softmax character proprobility with timesteps
outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)

# model to be used at test time

act_model = Model(inputs, outputs)

act_model.summary()

"""### Callback"""

### ctc definition part
# define the label input shape for ctc
labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')
# define the length of input and label for ctc
input_length = Input(name='input_length', shape=[1], dtype='int64')
label_length = Input(name='label_length', shape=[1], dtype='int64')

# define a ctc lambda function to take arguments and return ctc_bach_cost
def ctc_lambda_func(args):
  y_pred, labels, input_length, label_length = args
  """
  labels: tensor (number of samples, max_string_length) containing the truth labels.
  y_pred: tensor (number of samples, time_steps, num_character_labels) containing the prediction, or output of the softmax.
  input_length: tensor (number of samples, 1) containing the sequence length for each batch item in y_pred.
  label_length: tensor (number of samples, 1) containing the sequence length for each batch item in y_true.
  """
  return K.ctc_batch_cost(labels, y_pred, input_length, label_length)

# out loss function (just take the inputs and put it in our ctc_batch_cost)
loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])

#model to be used at training time
model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)

# ready ctc loss function and optimizers
model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam')

# our callbacks hell to optimize our learning
callbacks = [
    TensorBoard(
        log_dir=LOG_TENSORBOARD,
        histogram_freq=10,
        profile_batch=0,
        write_graph=True,
        write_images=False,
        update_freq="epoch"),
    ModelCheckpoint(
        filepath=os.path.join(BEST_CHECKPOINT_WEIGHT),
        monitor='val_loss',
        save_best_only=True,
        save_weights_only=True,
        verbose=1,
        mode='min'),
    EarlyStopping(
        monitor='val_loss',
        min_delta=0,
        patience=25,
        restore_best_weights=True,
        verbose=0,
        mode='min'),
    ReduceLROnPlateau(
        monitor='val_loss',
        min_delta=1e-8,
        factor=0.2,
        patience=20,
        verbose=1)
]
callbacks_list = callbacks

model.summary()

tf.keras.utils.plot_model(
    model,
    to_file=STRUCTURE_MODEL_IMG,
    show_shapes=True,
    show_dtype=True,
    show_layer_names=True,
    rankdir="TB",
    expand_nested=True,
    dpi=96,
    layer_range=None,
    show_layer_activations=True,
    show_trainable=True,
)

# ready our training data
training_img = np.array(training_img)
train_input_length = np.array(train_input_length)  # all must be equal length to T timesteps
train_label_length = np.array(train_label_length)  # different length (only the same in Captcha dataset)

# ready our validating data
valid_img = np.array(valid_img)
valid_input_length = np.array(valid_input_length) # all must be equal length to T timesteps
valid_label_length = np.array(valid_label_length) # different length (only the same in Captcha dataset)

training_img[:1]

valid_img[:1]

"""### Training"""

# choose batchsize and epochs

batch_size = 32
epochs = 100

history = model.fit(x=[training_img, train_padded_txt, train_input_length, train_label_length],
          y=np.zeros(len(training_img)),
          batch_size=batch_size,
          epochs = epochs,
          validation_data = ([valid_img, valid_padded_txt, valid_input_length, valid_label_length], [np.zeros(len(valid_img))]),
          verbose = 1, callbacks = callbacks_list)

model.save('/content/gdrive/MyDrive/CNTT2/DATASET/model/trained_model.h5')

"""## Evaluation
*   Character Error Rate
*   Word Error Rate
"""

# load the saved best model weights
act_model.load_weights(os.path.join(BEST_CHECKPOINT_WEIGHT))

NO_PREDICTS = 100
OFFSET = 0
prediction = act_model.predict(valid_img)

prediction.shape

# use CTC decoder
out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],
                         greedy=True)[0][0])
# see the results
all_predictions =[]
i = 0
for x in out:
  print("original_text  = ", valid_orig_txt[i+OFFSET])
  print("predicted text = ", end = '')
  pred = ""
  for p in x:
    if int(p) != -1:
      pred += char_list[int(p)]
  print(pred)
  all_predictions.append(pred)
  i += 1

import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
plt.figure(figsize=(10,200))
col=0
row=1
gs1 = gridspec.GridSpec(NO_PREDICTS, 1)
for n in range(NO_PREDICTS):
  plt.subplot(gs1[n])
  plt.imshow(valid_img[n][:,:,0], cmap="gray_r")
  plt.title(f"Label {n}: "+valid_orig_txt[n+OFFSET], fontsize=20, color="green")
  plt.xlabel(f"Prediction {n}: "+all_predictions[n+OFFSET], fontsize=20, color="red")

loss_train = history.history['loss']
loss_test = history.history['val_loss']

plt.plot(loss_train, label='Train Loss')
plt.plot(loss_test, label='Test Loss')
plt.title('Loss during training')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import string
import unicodedata
import editdistance

def ocr_metrics(predicts, ground_truth, norm_accentuation=False, norm_punctuation=False):
    if len(predicts) == 0 or len(ground_truth) == 0:
      return (1, 1)

    cer, wer = [], []

    for (pd, gt) in zip(predicts, ground_truth):

      if norm_accentuation:
        pd = unicodedata.normalize("NFKD", pd).encode("ASCII", "ignore").decode("ASCII")
        gt = unicodedata.normalize("NFKD", gt).encode("ASCII", "ignore").decode("ASCII")

      if norm_punctuation:
        pd = pd.translate(str.maketrans("", "", string.punctuation))
        gt = gt.translate(str.maketrans("", "", string.punctuation))

      pd_cer, gt_cer = list(pd.lower()), list(gt.lower())
      dist = editdistance.eval(pd_cer, gt_cer)
      cer.append(dist / (max(len(pd_cer), len(gt_cer))))

      pd_wer, gt_wer = pd.lower().split(), gt.lower().split()
      dist = editdistance.eval(pd_wer, gt_wer)
      wer.append(dist / (max(len(pd_wer), len(gt_wer))))

    cer_f = sum(cer) / len(cer)
    wer_f = sum(wer) / len(wer)

    return (cer_f, wer_f)

evaluate = ocr_metrics(predicts=all_predictions,
                                ground_truth=valid_orig_txt,
                                norm_accentuation=False,
                                norm_punctuation=False)
evaluate_string = "\n".join([
    "Metrics:",
    "Character Error Rate (CER): {}%".format(round(evaluate[0]*100, 2)),
    "Word Error Rate (WER):      {}%".format(round(evaluate[1]*100, 2)),
])

print(evaluate_string)

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir='/content/gdrive/MyDrive/CNTT2/DATASET/model/crnn/logs' --port=6767

"""# Translator"""

!pip install googletrans==4.0.0-rc1